import fs from 'fs'
import path from 'path'
import csvParser from 'csv-parser'
import db from './database'
import { importDuration } from '../app/api/metrics/route'
import { triggerDeltaUpdate, setCSVProcessingState } from '@/app/api/updates/route'
// Wetter-Imports entfernt
// import { fetchWeather } from './weather'
// import { insertWeather } from './db-helpers'

// Hilfsfunktion: Lese/Schreibe Offset-Metadaten f√ºr eine CSV-Datei
function getMetaPath(csvPath: string) {
  return csvPath + '.meta.json'
}
function readMeta(csvPath: string): { lastLine: number } {
  const metaPath = getMetaPath(csvPath)
  if (fs.existsSync(metaPath)) {
    try {
      return JSON.parse(fs.readFileSync(metaPath, 'utf-8'))
    } catch {
      return { lastLine: 0 }
    }
  }
  return { lastLine: 0 }
}
function writeMeta(csvPath: string, meta: { lastLine: number }) {
  const metaPath = getMetaPath(csvPath)
  fs.writeFileSync(metaPath, JSON.stringify(meta))
}

function writeMetaSync(csvPath: string, meta: { lastLine: number }) {
  const metaPath = getMetaPath(csvPath)
  fs.writeFileSync(metaPath, JSON.stringify(meta))
  // Crash-sicher: fsync nur bei explizitem Sync
  const fd = fs.openSync(metaPath, 'r')
  fs.fsyncSync(fd)
  fs.closeSync(fd)
}

// Hilfsfunktionen f√ºr CSV-Verarbeitung
function parseCSVLine(line: string, station: string, fileName: string): any {
  try {
    const parts = line.split(';')
    if (parts.length < 2) return null
    
    // Debug: Zeige alle Spalten f√ºr Analyse
    console.log(`üîç [CSV Processing] CSV line parts: ${parts.map(p => `"${p.trim()}"`).join(', ')}`)
    
    // Suche nach spezifischen Spaltennamen
    let sysTime = null
    let las = null
    let datum = null
    
    // Erste Zeile ist Header - √ºberspringe
    if (parts[0].toLowerCase().includes('messnummer') || parts[0].toLowerCase().includes('systemzeit')) {
      console.log(`üîç [CSV Processing] Skipping header row`)
      return null
    }
    
    // Suche nach Zeit-Spalte (verschiedene Namen)
    const timeColumns = ['Systemzeit', 'Systemzeit ', 'Zeit', 'Time', 'Uhrzeit']
    for (let i = 0; i < parts.length; i++) {
      const part = parts[i].trim()
      
      // Pr√ºfe ob es eine Zeit-Spalte ist
      if (timeColumns.some(col => part.includes(col) || part.match(/^\d{1,2}:\d{1,2}:\d{1,2}:\d{1,3}$/))) {
        // Extrahiere Zeit aus der Spalte - unterst√ºtzt Millisekunden
        const timeMatch = part.match(/(\d{1,2}):(\d{1,2}):(\d{1,2}):(\d{1,3})/)
        if (timeMatch) {
          // Entferne Millisekunden f√ºr Datenbank
          sysTime = `${timeMatch[1].padStart(2, '0')}:${timeMatch[2].padStart(2, '0')}:${timeMatch[3].padStart(2, '0')}`
          console.log(`üîç [CSV Processing] Found time: ${sysTime} (removed milliseconds)`)
        } else {
          // Fallback f√ºr Zeit ohne Millisekunden
          const timeMatch2 = part.match(/(\d{1,2}):(\d{1,2}):(\d{1,2})/)
          if (timeMatch2) {
            sysTime = `${timeMatch2[1].padStart(2, '0')}:${timeMatch2[2].padStart(2, '0')}:${timeMatch2[3].padStart(2, '0')}`
            console.log(`üîç [CSV Processing] Found time: ${sysTime}`)
          }
        }
      }
      
      // Suche nach L√§rm-Spalten (erweiterte Liste)
      const noiseColumns = ['LAS', 'LAF', 'LAeq', 'Lmax', 'Lmin', 'LAFT5s', 'LAFTeq', 'LAF5s', 'LCFeq', 'LCF5s', 'LS']
      for (const noiseCol of noiseColumns) {
        if (part.includes(noiseCol) || (!las && !isNaN(Number(part.replace(',', '.'))))) {
          const noiseValue = part.replace(',', '.')
          const noiseNum = Number(noiseValue)
          if (!isNaN(noiseNum) && noiseNum > 0 && noiseNum < 200) {
            las = noiseNum
            console.log(`üîç [CSV Processing] Found noise: ${las} from column ${noiseCol}`)
            break
          }
        }
      }
      
      // Suche nach Datum (DD.MM.YYYY Format)
      if (part.match(/^\d{2}\.\d{2}\.\d{4}$/)) {
        datum = part
        console.log(`üîç [CSV Processing] Found date: ${datum}`)
      }
    }
    
    if (!sysTime || !las) {
      console.log(`üîç [CSV Processing] Missing required data - time: ${sysTime}, noise: ${las}`)
      return null
    }
    
    // Datum bestimmen
    let dateStr = null
    if (datum) {
      const dateParts = datum.split('.')
      if (dateParts.length === 3) {
        dateStr = `${dateParts[2]}-${dateParts[1].padStart(2, '0')}-${dateParts[0].padStart(2, '0')}`
      }
    } else {
      // Datum aus Dateiname
      const dateMatch = fileName.match(/(\d{2})\.(\d{2})\.(\d{4})/)
      if (dateMatch) {
        dateStr = `${dateMatch[3]}-${dateMatch[2]}-${dateMatch[1]}`
      } else {
        // Fallback auf aktuelles Datum
        const now = new Date()
        dateStr = now.toISOString().split('T')[0]
      }
    }
    
    const datetime = `${dateStr} ${sysTime}`
    const time = datetime
    
    console.log(`üîç [CSV Processing] Parsed measurement: ${datetime}, noise: ${las}`)
    
    return {
      station,
      time,
      las,
      source_file: fileName,
      datetime,
      all_csv_fields: JSON.stringify({ line, station, fileName })
    }
  } catch (error) {
    console.error(`‚ùå [CSV Processing] Error parsing line: ${error}`)
    return null
  }
}

async function insertBatch(db: any, batches: any[], hasDatetime: boolean): Promise<number> {
  if (batches.length === 0) return 0
  
  try {
    const stmt = hasDatetime 
      ? db.prepare('INSERT OR REPLACE INTO measurements (station, time, las, source_file, datetime, all_csv_fields) VALUES (?, ?, ?, ?, ?, ?)')
      : db.prepare('INSERT OR REPLACE INTO measurements (station, time, las, source_file, all_csv_fields) VALUES (?, ?, ?, ?, ?)')
    
    let insertedCount = 0
    for (const batch of batches) {
      try {
        const result = hasDatetime
          ? stmt.run(batch.station, batch.time, batch.las, batch.source_file, batch.datetime, batch.all_csv_fields)
          : stmt.run(batch.station, batch.time, batch.las, batch.source_file, batch.all_csv_fields)
        
        if (result.changes > 0) {
          insertedCount++
        }
      } catch (error) {
        console.error(`‚ùå [CSV Processing] Error inserting batch: ${error}`)
      }
    }
    
    return insertedCount
  } catch (error) {
    console.error(`‚ùå [CSV Processing] Error in insertBatch: ${error}`)
    return 0
  }
}

// Streaming-CSV-Verarbeitung mit inkrementellem Offset
export async function processCSVFile(station: string, filePath: string): Promise<number> {
  const startTime = Date.now()
  
  try {
    console.log(`üîç [CSV Processing] Processing ${station}: ${path.basename(filePath)}`)
    
    // Debug: Pr√ºfe Dateigr√∂√üe
    const fileStat = fs.statSync(filePath)
    console.log(`üîç [CSV Processing] File size: ${fileStat.size} bytes`)
    
    // Debug: Pr√ºfe Offset-Datei
    const offsetFile = filePath.replace('.csv', '.meta.json')
    let offset = 0
    if (fs.existsSync(offsetFile)) {
      try {
        const offsetData = JSON.parse(fs.readFileSync(offsetFile, 'utf8'))
        offset = offsetData.offset || 0
        console.log(`üîç [CSV Processing] Found offset: ${offset} for ${path.basename(filePath)}`)
        
        // Force reprocess f√ºr techno und band (Debug-Modus)
        if (station === 'techno' || station === 'band') {
          console.log(`üîç [CSV Processing] Force reprocessing for ${station} - resetting offset`)
          offset = 0
          // L√∂sche Offset-Datei f√ºr Neuverarbeitung
          if (fs.existsSync(offsetFile)) {
            fs.unlinkSync(offsetFile)
            console.log(`üîç [CSV Processing] Deleted offset file for ${station}`)
          }
        }
      } catch (err) {
        console.log(`üîç [CSV Processing] Error reading offset file: ${err}`)
      }
    } else {
      console.log(`üîç [CSV Processing] No offset file found for ${path.basename(filePath)}`)
    }
    
    // Debug: Pr√ºfe Dateiinhalt
    const fileContent = fs.readFileSync(filePath, 'utf8')
    const lines = fileContent.split('\n').filter(line => line.trim())
    console.log(`üîç [CSV Processing] Total lines in file: ${lines.length}`)
    console.log(`üîç [CSV Processing] Lines after offset: ${lines.length - offset}`)
    
    if (lines.length <= offset) {
      console.log(`üîç [CSV Processing] File already fully processed (${lines.length} <= ${offset})`)
      return 0
    }
    
    // Debug: Zeige erste paar Zeilen
    const sampleLines = lines.slice(offset, Math.min(offset + 5, lines.length))
    console.log(`üîç [CSV Processing] Sample lines after offset:`)
    sampleLines.forEach((line, i) => {
      console.log(`üîç [CSV Processing] Line ${offset + i}: ${line.substring(0, 100)}...`)
    })
    
    const db = (await import('./database')).default
    
    // Pr√ºfe ob datetime Spalte existiert
    const columns = db.prepare("PRAGMA table_info(measurements)").all() as Array<{ name: string }>
    const hasDatetime = columns.some(col => col.name === 'datetime')
    console.log(`üîç [CSV Processing] Database has datetime column: ${hasDatetime}`)
    
    let insertedCount = 0
    const batchSize = 1000
    const batches = []
    
    // Verarbeite Zeilen ab dem Offset
    for (let i = offset; i < lines.length; i++) {
      const line = lines[i].trim()
      if (!line) continue
      
      try {
        const measurement = parseCSVLine(line, station, path.basename(filePath))
        if (measurement) {
          batches.push(measurement)
          
          if (batches.length >= batchSize) {
            const batchInserted = await insertBatch(db, batches, hasDatetime)
            insertedCount += batchInserted
            batches.length = 0
          }
        }
      } catch (error) {
        console.error(`‚ùå [CSV Processing] Error parsing line ${i + 1}: ${error}`)
        continue
      }
    }
    
    // F√ºge restliche Batches ein
    if (batches.length > 0) {
      const batchInserted = await insertBatch(db, batches, hasDatetime)
      insertedCount += batchInserted
    }
    
    // Speichere neuen Offset
    if (insertedCount > 0) {
      const newOffset = lines.length
      const offsetData = { offset: newOffset, lastProcessed: new Date().toISOString() }
      fs.writeFileSync(offsetFile, JSON.stringify(offsetData, null, 2))
      console.log(`üîç [CSV Processing] Updated offset to ${newOffset}`)
    }
    
    const durationSec = (Date.now() - startTime) / 1000
    console.log(`‚úÖ [CSV Processing] ${station}: ${path.basename(filePath)} - ${insertedCount} rows inserted in ${durationSec.toFixed(2)}s`)
    
    return insertedCount
  } catch (error) {
    console.error(`‚ùå [CSV Processing] Error processing ${filePath}:`, error)
    return 0
  }
}

export async function processAllCSVFiles(): Promise<number> {
  console.log('üîç Starte CSV-Verarbeitung f√ºr alle Stationen...')
  
  // Debug-Logging f√ºr Deployment
  console.log(`üîç [CSV Processing] Database path: ${process.env.DATABASE_PATH || './data.sqlite'}`)
  console.log(`üîç [CSV Processing] Current working directory: ${process.cwd()}`)
  console.log(`üîç [CSV Processing] NODE_ENV: ${process.env.NODE_ENV}`)
  console.log(`üîç [CSV Processing] Platform: ${process.platform}`)
  
  // SSE-Schutz aktivieren
  setCSVProcessingState(true)
  
  try {
    const stations = ['ort', 'techno', 'heuballern', 'band']
    let totalInserted = 0
    
    for (const station of stations) {
      const csvDir = path.join(process.cwd(), 'public', 'csv', station)
      console.log(`üìÅ √úberpr√ºfe Verzeichnis: ${csvDir}`)
      
      if (!fs.existsSync(csvDir)) {
        console.log(`üìÅ Verzeichnis nicht gefunden: ${csvDir}`)
        continue
      }
      
      const csvFiles = fs.readdirSync(csvDir).filter(file => file.endsWith('.csv'))
      console.log(`üìä Gefunden: ${csvFiles.length} CSV-Dateien in ${station}`)
      
      // Debug: Zeige die ersten paar CSV-Dateien
      if (csvFiles.length > 0) {
        console.log(`üìÑ Erste CSV-Dateien in ${station}:`, csvFiles.slice(0, 3))
      }
      
      for (const csvFile of csvFiles) {
        const csvPath = path.join(csvDir, csvFile)
        console.log(`üìÑ Processing: ${csvPath}`)
        
        // Debug: Pr√ºfe Dateigr√∂√üe
        try {
          const stats = fs.statSync(csvPath)
          console.log(`üìä Dateigr√∂√üe: ${stats.size} bytes`)
        } catch (err) {
          console.error(`‚ùå Fehler beim Lesen der Datei: ${csvPath}`, err)
        }
        
        const insertedCount = await processCSVFile(station, csvPath)
        totalInserted += insertedCount
        console.log(`üìä ${station}: ${csvFile} - ${insertedCount} rows inserted`)
      }
    }
    
    console.log(`üìä Insgesamt ${totalInserted} Messwerte importiert`)
    
    // Debug: √úberpr√ºfe Datenbank nach Import
    try {
      const db = (await import('./database')).default
      const result = db.prepare('SELECT station, COUNT(*) as count FROM measurements GROUP BY station').all()
      console.log(`üîç [CSV Processing] Database contents after import:`, result)
      
      // Debug: Pr√ºfe spezifisch techno und band
      const technoCount = db.prepare('SELECT COUNT(*) as count FROM measurements WHERE station = ?').get('techno') as { count: number } | undefined
      const bandCount = db.prepare('SELECT COUNT(*) as count FROM measurements WHERE station = ?').get('band') as { count: number } | undefined
      console.log(`üîç [CSV Processing] Techno count: ${technoCount?.count}, Band count: ${bandCount?.count}`)
      
    } catch (dbError) {
      console.error(`‚ùå [CSV Processing] Error checking database:`, dbError)
    }
    
    return totalInserted
  } finally {
    // SSE-Schutz deaktivieren
    setCSVProcessingState(false)
  }
}

if (require.main === module) {
  processAllCSVFiles().then(count => {
    console.log(`CSV-Import abgeschlossen. Gesamt: ${count}`)
    process.exit(0)
  }).catch(e => {
    console.error('Fehler beim CSV-Import:', e)
    process.exit(1)
  })
}